import os
import sys
import requests
import subprocess

from bitextor.utils.args import validate_args
from bitextor.utils.common import (
    open_xz_or_gzip_or_plain,
    get_all_ppids,
    snake_no_more_race_get_pgid,
    snake_no_more_race_get,
    snake_no_more_race_set,
)
from bitextor.utils.non_generic import *

valid, config = validate_args(config)
assert valid  # workaround for not being able to exit() in snakemake

#################################################################
# BASIC PARAMETERS

WORKFLOW = workflow.basedir
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

LANGS = set()
LANG1 = ""
LANG2 = ""

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANG1 = config["lang1"]
    LANGS.add(LANG1)
if "lang2" in config:
    LANG2 = config["lang2"]
    LANGS.add(LANG2)

PROFILING = ""
if "profiling" in config and config["profiling"]:
    PROFILING = "/usr/bin/time -v"

#################################################################
# CRAWLING
CRAWLTARGET = "wget"
TLD_CRAWL = ""
USERAGENT = ""
CRAWLSIZELIMIT = ""
CRAWLTIMELIMIT = ""
CRAWLWAIT = ""
CRAWLPAGELIMIT = ""
CRAWLFILETYPES = "-f html,pdf"
CRAWLJOBS = "-j 2"
CRAWLTIMEOUT = ""
CRAWLDUMPARGS = ""
CONTINUECRAWL = ""
CRAWLMAXFOLDERTREEDEPTH = ""
CRAWLSCOUTSTEPS = ""
CRAWLBLACKLISTURL = (
    "['wordpress','blogspot','facebook','google','wikipedia','youtube','perehodi','twitter','instagram']"
)
CRAWLPREFIXFILTER = "['mailto:']"

if "crawler" in config:
    CRAWLTARGET = config["crawler"]

if "crawlTLD" in config and config["crawlTLD"]:
    TLD_CRAWL = "-D"
if "crawlerUserAgent" in config:
    USERAGENT = f'-a "{config["crawlerUserAgent"]}"'
if "crawlSizeLimit" in config:
    CRAWLSIZELIMIT = f'-s {config["crawlSizeLimit"]}'
if "crawlTimeLimit" in config:
    CRAWLTIMELIMIT = f'-t {config["crawlTimeLimit"]}'
if "crawlWait" in config:
    CRAWLWAIT = f'--wait {config["crawlWait"]}'
if "crawlFileTypes" in config:
    CRAWLFILETYPES = f'-f {config["crawlFileTypes"]}'
if "crawlerNumThreads" in config:
    CRAWLJOBS = f'-j {config["crawlerNumThreads"]}'
if "crawlerConnectionTimeout" in config:
    CRAWLTIMEOUT = f'-o {config["crawlerConnectionTimeout"]}'
if "dumpCurrentCrawl" in config:
    CRAWLDUMPARGS = f'-d {config["dumpCurrentCrawl"]}'
if "resumePreviousCrawl" in config:
    CONTINUECRAWL = f'-l {config["resumePreviousCrawl"]}'
if "crawlMaxFolderTreeDepth" in config:
    CRAWLMAXFOLDERTREEDEPTH = config["crawlMaxFolderTreeDepth"]
if "crawlScoutSteps" in config:
    CRAWLSCOUTSTEPS = config["crawlScoutSteps"]
if "crawlBlackListURL" in config:
    CRAWLBLACKLISTURL = str(config["crawlBlackListURL"])
if "crawlPrefixFilter" in config:
    CRAWLPREFIXFILTER = str(config["crawlPrefixFilter"])

#################################################################
# PREPROCESS
PPROC = "warc2text"
PPROC_FILES = ["text.gz", "url.gz", "mime.gz"]
TEXT_FILE = "text.gz"
HTML_FILE = ""
if "writeHTML" in config and config["writeHTML"]:
    HTML_FILE = "html.gz"
    PPROC_FILES.append("html.gz")

SHARDS = config["shards"]
BATCHES = config["batches"]

CLEANHTML = ""
FTFY = ""
LANGID = "cld2"
PARSER = ""
BOILERPIPE = ""
PDFEXTRACT = ""
HTML5LIB = ""

if "cleanHTML" in config and config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
if "ftfy" in config and config["ftfy"]:
    FTFY = "--ftfy"
if "langID" in config:
    LANGID = config["langID"]
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "boilerpipeCleaning" in config and config["boilerpipeCleaning"] == True:
    BOILERPIPE = "--boilerpipe"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT = "--pdfextract "
    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT += " --pe_configfile " + config["PDFextract_configfile"]
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT += " --sentence_join_path " + config["PDFextract_sentence_join_path"]
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT += " --kenlm_path " + config["PDFextract_kenlm_path"]
if "html5lib" in config and config["html5lib"]:
    HTML5LIB = "--html5lib"

# sentence splitting and tokenisation
SENTTOKS = {} if not "sentenceSplitters" in config else config["sentenceSplitters"]
CUSTOMNBPS = {} if not "customNBPs" in config else config["customNBPs"]
WORDTOKS = {} if not "wordTokenizers" in config else config["wordTokenizers"]
MORPHTOKS = {} if not "morphologicalAnalysers" in config else config["morphologicalAnalysers"]

# Concrete tokenizers and morph. tokenizers
if WORDTOKS:
    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)
    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)
else:
    # TODO: decide where to put this
    WORDTOK1, WORDTOK2 = (
        f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG1}",
        f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG2}",
    )
MORPHTOK1 = get_lang_or_default(MORPHTOKS, LANG1)
MORPHTOK2 = get_lang_or_default(MORPHTOKS, LANG2)

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

# Sharding
EMPTY_SHARD_CHECK = f"{TMPDIR}/empty_shard"

#################################################################
# DOCALIGN
DOCALIGN = "neural"
DOC_THRESHOLD = 0.7

if "documentAlignerThreshold" in config:
    DOC_THRESHOLD = config["documentAlignerThreshold"]

#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
#################################################################
# CLEANING
FIELDS = ["url1", "url2", "seg1", "seg2", "aligner"]
MMHSUM_PATH = ""
BIFIXER = False
BIFIXER_FIELDS = []
AGGRESSIVE_DEDUP = "--aggressive_dedup"
BICLEANER = False
BICLEANER_MODEL = ""
BICLEANER_FIELDS = []
BICLEANER_THRESHOLD = 0.0
ELRC = False
ELRC_FIELDS = []
TMX = False
DEDUPED = False
BIROAMER = False
BIROAMER_MIX_FILES = "/dev/null"
BIROAMER_ALIGNMENT_CORPUS = ""
BIROAMER_OMIT = ""
# TODO: add rawCorpus option to generate lang1-lang2.raw.gz
OUTPUT_FILES = ["sent", "raw"]
BICLEANER_TRAIN_PREFIXES = None

if "bifixer" in config and config["bifixer"]:
    BIFIXER = True
    BIFIXER_FIELDS = ["bifixerhash", "bifixerscore"]

if "aggressiveDedup" in config and not config["aggressiveDedup"]:
    AGGRESSIVE_DEDUP = ""
if "bicleaner" in config:
    BICLEANER = True
    BICLEANER_MODEL = config["bicleaner"]
    BICLEANER_FIELDS = ["bicleaner"]
if "bicleanerThreshold" in config:
    BICLEANER_THRESHOLD = config["bicleanerThreshold"]
if "bicleanerCorpusTrainingPrefix" in config:
    BICLEANER_TRAIN_PREFIXES = config["bicleanerCorpusTrainingPrefix"]
if "elrc" in config and config["elrc"]:
    ELRC = True
    ELRC_FIELDS = ["lengthratio", "numTokensSL", "numTokensTL"]
if "tmx" in config and config["tmx"]:
    TMX = True
    OUTPUT_FILES.append("not-deduped.tmx")
if "deduped" in config and config["deduped"]:
    DEDUPED = True
    OUTPUT_FILES.append("deduped.tmx")
    OUTPUT_FILES.append("deduped.txt")
if "biroamer" in config and config["biroamer"]:
    BIROAMER = True

    if "deduped.tmx" in OUTPUT_FILES:
        OUTPUT_FILES.append("deduped-roamed.tmx")
    else:
        OUTPUT_FILES.append("not-deduped-roamed.tmx")

    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:
        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])

    if "biroamerImproveAlignmentCorpus" in config:
        BIROAMER_ALIGNMENT_CORPUS = f"-a {config['biroamerImproveAlignmentCorpus']}"

    if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]:
        BIROAMER_OMIT = "-o"

BEFORE_ELRC_FIELDS = FIELDS + BIFIXER_FIELDS + BICLEANER_FIELDS
TMX_FIELDS = BEFORE_ELRC_FIELDS + ELRC_FIELDS

FILTER_SORT_FIELDS = "-k3,4"
TMX_DEDUP_FIELDS = "seg1,seg2"
if "bifixerhash" in BEFORE_ELRC_FIELDS:
    i = BEFORE_ELRC_FIELDS.index("bifixerhash")
    i = i + 1  # sort counts from 1, not 0
    FILTER_SORT_FIELDS = f"-k{i},{i} -k{i+1},{i+1}nr"
    TMX_DEDUP_FIELDS = "bifixerhash"

BEFORE_ELRC_FIELDS = ",".join(BEFORE_ELRC_FIELDS)
TMX_FIELDS = ",".join(TMX_FIELDS)
#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])

if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])

if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())

if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())

DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)
# process WARCs individually
TARGET_2_WARCS = {f"{k}": v for k, v in enumerate(WARCS)}
# group crawled hosts by domains
TARGET_2_WARCS.update(
    dict([
        (domain, [f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz" for host in hosts])
        for (domain, hosts) in DOMAIN_2_HOSTS.items()
    ])
)

TARGETS = TARGET_2_WARCS.keys()

#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {
    "split": 1,
    "docalign": 1,
    "segalign": 1,
    "bifixer": 1,
    "bicleaner": 1,
    "sents": 1,
}
if "parallelWorkers" in config:
    for k in config["parallelWorkers"]:
        THREADS[k] = config["parallelWorkers"][k]

OUTPUT = []
UNTIL = config["until"] if "until" in config else ""
if "until" not in config:
    OUTPUT = expand(
        "{permanent}/{lang1}-{lang2}.{output_file}.gz",
        permanent=PERMANENT,
        target=TARGETS,
        lang1=LANG1,
        lang2=LANG2,
        output_file=OUTPUT_FILES,
    )
elif UNTIL == "crawl":
    for domain, hosts in DOMAIN_2_HOSTS.items():
        for host in hosts:
            OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")
elif UNTIL == "preprocess":
    OUTPUT = expand(
        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGETS,
        pproc=PPROC,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    )
elif UNTIL == "shard":
    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "split":
    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)
else:
    if UNTIL == "docalign":
        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")
    elif UNTIL == "segalign":
        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")
    elif UNTIL == "bifixer":
        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")
    elif UNTIL == "bicleaner":
        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")
    elif UNTIL == "filter":
        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")

shell.prefix("set -euo pipefail;")


#################################################################
### FINAL OUTPUTS ###############################################
rule all:
    input:
        OUTPUT,


#################################################################
### CRAWLING ####################################################


rule wget_download:
    params:
        url="http://{target}",
        folder=f"{DATADIR}/warc/{{target}}",
    output:
        f"{DATADIR}/warc/{{target}}/wget.warc.gz",
    shell:
        """
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} python3 {WORKFLOW}/bitextor_wget.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {USERAGENT} {CRAWLFILETYPES} {CRAWLWAIT} --warc {output}
        rm -rf $DIRNAME
        """


#################################################################
### PREPROCESS ##################################################

# pproc_output = {}
# for pproc_file in PPROC_FILES:
#     name = pproc_file.split('.')[0]
#     for lang in LANGS:
#         pproc_output[f"{lang}_{name}"] = f"{DATADIR}/preprocess/{{target}}/{PPROC}/{lang}/{pproc_file}"


def get_warcs_names(wildcards):
    return TARGET_2_WARCS[wildcards.target].split("/")[-1]


def get_pproc_input(wildcards):
    return TARGET_2_WARCS[wildcards.target]


rule warc2text:
    input:
        get_pproc_input,
    output:
        expand(
            "{data}/preprocess/{{target}}/warc2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=f"{DATADIR}/preprocess/{{target}}/warc2text",
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
    shell:
        """
        mkdir -p {params.folder}
        {PROFILING} warc2text -o {params.folder} -s -f {params.f} {input}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch {EMPTY_SHARD_CHECK}
            fi
        done
        """


def get_shard_input(lang):
    return expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=TARGETS, pproc=PPROC)


# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    # use url.gz as input to avoid having directories as input
    input:
        get_shard_input,
    output:
        f"{DATADIR}/shards/02.batches.{{lang}}",  # list of batches created for lang
    params:
        n=SHARDS,
        b=BATCHES,
        o_no_lang=f"{DATADIR}/shards",
        o=f"{DATADIR}/shards/{{lang}}",
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        empty_shard_batch_folder="EMPTY/1",
    shell:
        """
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run

        binary=giashard
        if [[ "$(command -v $binary)" == "" ]]; then
            binary=~/go/bin/giashard
        fi

        {PROFILING} $binary -n {params.n} -b {params.b} -o {params.o} -f {params.f} {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang}

        nofiles=$(ls {params.o} | wc -l)
        if [ "$nofiles" == "0" ]; then
            # No files generated
            >&2 echo "WARNING: no files generated after running giashard (lang '{wildcards.lang}'). Creating empty files instead"
        fi

        if [[ -f "{EMPTY_SHARD_CHECK}" ]]; then
            # Generate empty shards if needed in order to avoid the pipeline to break
            mkdir -p {params.o}/{params.empty_shard_batch_folder}

            touch {params.o}/{params.empty_shard_batch_folder}/empty
            touch {params.o}/{params.empty_shard_batch_folder}/{{{params.f}}}
            gzip {params.o}/{params.empty_shard_batch_folder}/{{{params.f}}}
        fi

        ls -d {params.o}/*/* > {output}
        """


# obtain list of batches for lang
def get_batches(lang):
    batches = []
    with checkpoints.shard.get(lang=lang).output[0].open() as f:
        for line in f:
            batches.append(line.strip())
    return batches


def apply_format(string, replace_format, replace_token="{}", replace_only_if_true=True):
    if replace_only_if_true and string or not replace_only_if_true:
        return replace_format.replace(replace_token, string)

    return string


rule split:
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/{TEXT_FILE}",
    params:
        splitter=lambda wildcards: apply_format(
            get_lang_or_default(SENTTOKS, wildcards.lang), '--sentence-splitter "{}"'
        ),
        customnbp=lambda wildcards: apply_format(get_customnbp(CUSTOMNBPS, wildcards.lang), '--customnbp "{}"'),
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz",
    threads: THREADS["split"]
    shell:
        """
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_split.py \
                {params.splitter} {params.customnbp} \
                --langcode "{wildcards.lang}" \
                {PRUNE_THRESHOLD} {PRUNE_TYPE} \
            | pigz -c > {output}
        """


rule aggregate_split:
    input:
        lambda wildcards: [f"{batch}/sentences.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/03.split.{{lang}}",
    shell:
        """ echo "{input}" | tr ' ' '\n' > {output} """


#################################################################
### DOCALIGN ####################################################
def get_align_inputs(src_lang, trg_lang):
    src_batches = get_batches(src_lang)
    trg_batches = get_batches(trg_lang)
    # each input -> (shard, (src_batch, trg_batch))
    inputs = get_docalign_inputs(src_batches, trg_batches)
    return inputs


rule aggregate_matches:
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{shard}/{LANG1}{src_batch}_{LANG2}{trg_batch}.06_01.matches"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(LANG1, LANG2)
        ],
    output:
        f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule doc_emb_matches:
    input:
        l1=f"{DATADIR}/shards/{LANG1}/{{shard}}/{{src_batch}}/sentences.gz",
        l2=f"{DATADIR}/shards/{LANG2}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/{{shard}}/{LANG1}{{src_batch}}_{LANG2}{{trg_batch}}.06_01.matches",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{{shard}}",
    threads: THREADS["docalign"]
    shell:
        """
        mkdir -p {params.folder}
        cat <(zcat {input.l1} | sed "s:$:	-	src:") <(zcat {input.l2} | sed "s:$:	-	trg:") | {PROFILING} neural-document-aligner - {params.folder}/emb_src {params.folder}/emb_trg \
                        --docalign-strategy 'faiss' --weights-strategy 0 \
                        --merging-strategy 3 --results-strategy 0 \
                        --emb-optimization-strategy 2 --gen-emb-optimization-strategy 2 \
                        --output-with-idxs --paths-to-docs-are-base64-values \
                        --threshold {DOC_THRESHOLD} > {output}
        """


### SEGALIGN ####################################################
rule aggregate_segalign:
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{shard}/{LANG1}{src_batch}_{LANG2}{trg_batch}.06_02.segalign.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(LANG1, LANG2)
        ],
    output:
        f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# VECALIGN #####################################################
rule vecalign:
    input:
        indices=rules.doc_emb_matches.output,
        plain1=f"{DATADIR}/shards/{LANG1}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{LANG2}/{{shard}}/{{trg_batch}}/sentences.gz",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{{shard}}/vecalign",
        workers=THREADS["segalign"],
        vecalign_dir=f"{WORKFLOW}/vecalign",
    output:
        temp(
            f"{TRANSIENT}/{LANG1}_{LANG2}/{{shard}}/{LANG1}{{src_batch}}_{LANG2}{{trg_batch}}.06_02.segalign.gz"
        ),
    threads: max(THREADS["segalign"], 2)
    shell:
        """
        mkdir -p "{params.folder}"
        parallel_cmd=""
        if [ {params.workers} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params.workers} -l 1 --group"
        fi

        cat <(zcat {input.plain1} | sed "s:$:	-	src:") <(zcat {input.plain2} | sed "s:$:	-	trg:") \
            | {PROFILING} LASER="{WORKFLOW}/LASER" python3 "{WORKFLOW}/bitextor_nda_vecalign.py" - "{input.indices}" "{params.vecalign_dir}" \
                --tmp-dir "{params.folder}" --src-lang {LANG1} --trg-lang {LANG2} --nda-input-is-base64 \
            | pigz -c > "{output}"
        """


### FILTERING AND CLEANING ######################################

split_input_filename = "06_02.segalign"
split_input_extension = ".gz"

if SEGALIGN == "hunalign":
    split_input_filename = "hunalign.06_02.segalign"
    split_input_extension = ".xz"


# split segalign results into balanced chunks
checkpoint split_segalign:
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{shard}/{LANG1}{src_batch}_{LANG2}{trg_batch}.{split_input_filename}{split_input_extension}"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(LANG1, LANG2)
        ],
    output:
        batches=f"{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches",
    params:
        size=BATCHES,  # use same parameter as for shards
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}",
    run:
        # We need to make this check in order to avoid the pipeline to break in the case that no input was received (e.g. no common shards)
        # It is needed to run this piece of code directly in Python and not in bash because Snakemake attempts to replace {input[0]} before
        #  running the script, so it fails even before we are able to check if the input is empty in bash script
        if len(input) == 0:
            shell(
                f"""
                >&2 echo "INFO: no data to work with. Stopping execution"
                touch {output}
                # Kill only the current Snakemake, not all of them (might be running multiple instances, and we only want to stop this one)
                # https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-do-i-exit-a-running-snakemake-workflow
                pid_to_kill="{snake_no_more_race_get_pgid()}"
                if [[ "$pid_to_kill" == "" ]]; then
                    echo "ERROR: could not stop the execution with a normal status. Forcing snakemake to stop"
                    exit 1
                else
                    kill -TERM "$pid_to_kill" || exit 1
                fi
                """
            )
        else:
            shell(
                """
                mkdir -p {params.folder}
                rm -f {params.folder}/* # remove anything that might be left after a previous run
                CAT=cat
                if [[ {input[0]} == *.gz ]]; then
                    CAT=zcat
                elif [[ {input[0]} == *.xz ]]; then
                    CAT=xzcat
                fi
                $CAT {input} \
                    | ( [ "{LANG1}" = "{LANG1}" ] && cat || awk -F '\t' '{{ print $2,$1,$4,$3,$5 }}' OFS='\t' )\
                    | python3 {WORKFLOW}/bitextor_split_segalign.py -f 3,4 -s {params.size} --gzip -o "{params.folder}/"
                if [ -z "$(ls -A {params.folder})" ]; then
                    cat < /dev/null > {output.batches}
                else
                    ls {params.folder}/* | sed 's/.gz$//g' > {output.batches}
                fi
                """
            )


def get_postproc_batches():
    batches = []
    with checkpoints.split_segalign.get().output.batches.open() as f:
        for line in f:
            batches.append(line.strip().split("/")[-1])  # obtain just the number of the chunks
    return batches


rule bifixer:
    input:
        segalign=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz",
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}"),
    threads: THREADS["bifixer"]
    shell:
        """
        CAT=cat; if [[ {input.segalign} == *.gz ]]; then CAT=zcat; fi
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        $CAT {input.segalign} \
            | {PROFILING} ${{parallel_cmd}} bifixer -q - - {LANG1} {LANG2} {AGGRESSIVE_DEDUP} \
            > {output}
        """


rule aggregate_bifixer:
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


bicleaner_input = rules.bifixer.output
if not BIFIXER:
    bicleaner_input = rules.bifixer.input.segalign


rule bicleaner:
    input:
        bifixer=bicleaner_input,
        model=BICLEANER_MODEL,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz",
    params:
        THREADS["bicleaner"],
    threads: max(2, THREADS["bicleaner"])
    shell:
        """
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        CAT=cat; if [[ {input.bifixer} == *.gz ]]; then CAT=zcat; fi
        slang=$(egrep "source_lang" {input.model} | cut -d " " -f 2)
        if [ "$slang" == "{LANG1}" ]; then
            $CAT {input.bifixer} \
                | {PROFILING} cache -k 3,4 ${{parallel_cmd}} bicleaner-classify-lite --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        else
            $CAT {input.bifixer} \
                | awk ' BEGIN {{FS="\t"; OFS="\t"}} {{ t = $3; $3 = $4; $4 = t; print;}} ' \
                | {PROFILING} cache -k 3,4 ${{parallel_cmd}} bicleaner-classify-lite --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        fi
        """


rule aggregate_bicleaner:
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


filter_input = rules.bicleaner.output
if not BICLEANER:
    filter_input = rules.bicleaner.input.bifixer


rule filter:
    input:
        filter_input,
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}"),
    threads: lambda wildcards: 2 if BICLEANER and ELRC else 1
    run:
        cat_cmd = "cat"
        if input[0][-3:] == ".gz":
            cat_cmd = "zcat"
        cmd = f""" {cat_cmd} {input} """
        if BICLEANER:
            cmd += (
                f""" | {PROFILING} python3 {WORKFLOW}/bitextor_filterbicleaner.py --threshold {BICLEANER_THRESHOLD} """
            )
        if ELRC:
            cmd += f""" | {PROFILING} python3 {WORKFLOW}/bitextor_elrc_filtering.py -c "{BEFORE_ELRC_FIELDS}" -s """
        cmd += f""" | LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} """  # sorted by either sentences or bifixer
        cmd += f""" > {output} """
        shell(cmd)


rule aggregate_filter:
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


raw_input_filename = rules.filter.input[0].split("/")[-2]  # 06_02.segalign / 07_01.bifixer / 07_02.bicleaner
extension = ""
if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]:
    extension = ".gz"


rule raw:
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}{extension}" for batch in get_postproc_batches()
        ],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.raw.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.raw",
    shell:
        """
        if [[ {input[0]} == *.gz ]]; then
            cat {input} > {output.corpus}
        else
            cat {input} | pigz -c > {output.corpus}
        fi
        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.corpus} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        """


rule sents:
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.sent.gz",
    threads: THREADS["sents"]
    shell:
        """
        LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} --parallel {threads} --compress-program=gzip -T {TMPDIR} --merge {input} \
            | pigz -c > {output}
        """


rule tmx:
    input:
        rules.sents.output,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz",
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" \
            | pigz -c > {output}
        """


rule deduped_tmx:
    input:
        rules.sents.output,
    output:
        tmx=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz",
        txt=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.deduped",
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" --dedup "{TMX_DEDUP_FIELDS}" -f {output.txt} \
            | pigz -c > {output.tmx}
        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.txt} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        """


rule roam_tmx:
    input:
        tmx=rules.tmx.output if not DEDUPED else rules.deduped_tmx.output.tmx,
        txt=rules.sents.output if not DEDUPED else rules.deduped_tmx.output.txt,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped-roamed.tmx.gz" if not DEDUPED else f"{PERMANENT}/{LANG1}-{LANG2}.deduped-roamed.tmx.gz",
    params:
        tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG1), '-t "{}"'),
        tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG2), '-T "{}"'),
    shell:
        """
        mix_files="$(mktemp {TMPDIR}/roam_tmx_mix.{LANG1}-{LANG2}.XXXXXX)"
        cat {BIROAMER_MIX_FILES} > $mix_files
        nolines=$(cat $mix_files | wc -l)

        if [[ "$nolines" != "0" ]]; then
            mix_files="-m $mix_files"
        else
            mix_files=""
        fi

        CAT=cat; if [[ {input.txt} == *.gz ]]; then CAT=zcat; fi
        nolines_txt=$($CAT {input.txt} | wc -l)
        total_nolines=$(echo $nolines + $nolines_txt | bc)

        if [[ "$total_nolines" -lt "100000" ]] && [[ "{BIROAMER_ALIGNMENT_CORPUS}" == "" ]]; then
            >&2 echo "WARNING: biroamer suggests, at least, 100k lines in order to have a good alignment (current nolines: $total_nolines)."\
                     "Check 'biroamerImproveAlignmentCorpus' config option if you want to improve the resulted roamed tmx"
        fi

        zcat {input.tmx} \
            | {PROFILING} biroamer {params.tokenizer_l1} {params.tokenizer_l2} \
                {BIROAMER_OMIT} {BIROAMER_ALIGNMENT_CORPUS} $mix_files {LANG1} {LANG2} \
            | pigz -c > {output}
        """
